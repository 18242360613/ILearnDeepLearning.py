{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Conv + Pool +Flatten + Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.activation.relu import ReluLayer\n",
    "from src.layers.pooling import MaxPoolLayer\n",
    "from src.activation.softmax import SoftmaxLayer\n",
    "from src.layers.dense import DenseLayer\n",
    "from src.layers.flatten import FlattenLayer\n",
    "from src.layers.convolutional import ConvLayer2D\n",
    "from src.model.sequential import SequentialModel\n",
    "from src.utils.core import convert_categorical2one_hot, convert_prob2categorical\n",
    "from src.utils.metrics import calculate_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the train data set\n",
    "N_TRAIN_SAMPLES = 5000\n",
    "# number of samples in the test data set\n",
    "N_TEST_SAMPLES = 500\n",
    "# number of samples in the validation data set\n",
    "N_VALID_SAMPLES = 500\n",
    "# number of classes\n",
    "N_CLASSES = 10\n",
    "# image size\n",
    "IMAGE_SIZE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Build data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape: (60000, 28, 28)\n",
      "trainY shape: (60000,)\n",
      "testX shape: (10000, 28, 28)\n",
      "testY shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
    "print(\"trainX shape:\", trainX.shape)\n",
    "print(\"trainY shape:\", trainY.shape)\n",
    "print(\"testX shape:\", testX.shape)\n",
    "print(\"testY shape:\", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX[:N_TRAIN_SAMPLES, :, :]\n",
    "y_train = trainY[:N_TRAIN_SAMPLES]\n",
    "\n",
    "X_test = trainX[N_TRAIN_SAMPLES:N_TRAIN_SAMPLES+N_TEST_SAMPLES, :, :]\n",
    "y_test = trainY[N_TRAIN_SAMPLES:N_TRAIN_SAMPLES+N_TEST_SAMPLES]\n",
    "\n",
    "X_valid = testX[:N_VALID_SAMPLES, :, :]\n",
    "y_valid = testY[:N_VALID_SAMPLES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** We need to change the data format to the shape supported by my implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (28, 28, 1, 5000)\n",
      "y_train shape: (10, 5000)\n",
      "X_test shape: (28, 28, 1, 500)\n",
      "y_test shape: (10, 500)\n",
      "X_valid shape: (28, 28, 1, 500)\n",
      "y_valid shape: (10, 500)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.moveaxis(X_train, 0, -1) / 255\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "y_train = convert_categorical2one_hot(y_train).T\n",
    "X_test = np.moveaxis(X_test, 0, -1) / 255\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "y_test = convert_categorical2one_hot(y_test).T\n",
    "X_valid = np.moveaxis(X_valid, 0, -1) / 255\n",
    "X_valid = np.expand_dims(X_valid, axis=2)\n",
    "y_valid = convert_categorical2one_hot(y_valid).T\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    ConvLayer2D.initialize(filters=16, kernel_shape=(3, 3, 1)), # input (28, 28, 1, N) out (26, 26, 16, N)\n",
    "    ReluLayer(), # input (26, 26, 16, N) out (26, 26, 16, N)\n",
    "    MaxPoolLayer(pool_size=(2, 2), strides=2), # input (26, 26, 16, N) out (13, 13, 16, N)\n",
    "    ConvLayer2D.initialize(filters=32, kernel_shape=(5, 5, 16)), # input (13, 13, 16, N) out (9, 9, 32, N)\n",
    "    ReluLayer(), # input (9, 9, 32, N) out (9, 9, 32, N)\n",
    "    MaxPoolLayer(pool_size=(3, 3), strides=3), # input (9, 9, 32, N) out (3, 3, 32, N)\n",
    "    FlattenLayer(), # input (3, 3, 32, N) out (288, N)\n",
    "    DenseLayer.initialize(input_dim=288, output_dim=256),\n",
    "    ReluLayer(),\n",
    "    DenseLayer.initialize(input_dim=256, output_dim=256),\n",
    "    ReluLayer(),\n",
    "    DenseLayer.initialize(input_dim=256, output_dim=128),\n",
    "    ReluLayer(),\n",
    "    DenseLayer.initialize(input_dim=128, output_dim=64),\n",
    "    ReluLayer(),\n",
    "    DenseLayer.initialize(input_dim=64, output_dim=N_CLASSES),\n",
    "    SoftmaxLayer()\n",
    "]\n",
    "\n",
    "model = SequentialModel(layers=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 - cost: 2.30212 - accuracy: 0.14800\n",
      "Iteration: 00001 - cost: 2.27781 - accuracy: 0.14800\n",
      "Iteration: 00002 - cost: 2.25852 - accuracy: 0.16600\n",
      "Iteration: 00003 - cost: 2.24184 - accuracy: 0.20200\n",
      "Iteration: 00004 - cost: 2.22653 - accuracy: 0.24600\n",
      "Iteration: 00005 - cost: 2.21163 - accuracy: 0.25400\n",
      "Iteration: 00006 - cost: 2.19663 - accuracy: 0.26200\n",
      "Iteration: 00007 - cost: 2.18145 - accuracy: 0.26200\n",
      "Iteration: 00008 - cost: 2.16603 - accuracy: 0.27000\n",
      "Iteration: 00009 - cost: 2.15047 - accuracy: 0.27400\n",
      "Iteration: 00010 - cost: 2.13464 - accuracy: 0.28000\n",
      "Iteration: 00011 - cost: 2.11826 - accuracy: 0.28800\n",
      "Iteration: 00012 - cost: 2.10124 - accuracy: 0.29800\n",
      "Iteration: 00013 - cost: 2.08358 - accuracy: 0.31000\n",
      "Iteration: 00014 - cost: 2.06531 - accuracy: 0.32400\n",
      "Iteration: 00015 - cost: 2.04652 - accuracy: 0.34200\n",
      "Iteration: 00016 - cost: 2.02728 - accuracy: 0.35000\n",
      "Iteration: 00017 - cost: 2.00770 - accuracy: 0.36000\n",
      "Iteration: 00018 - cost: 1.98777 - accuracy: 0.37000\n",
      "Iteration: 00019 - cost: 1.96748 - accuracy: 0.37400\n",
      "Iteration: 00020 - cost: 1.94678 - accuracy: 0.37400\n",
      "Iteration: 00021 - cost: 1.92560 - accuracy: 0.37400\n",
      "Iteration: 00022 - cost: 1.90393 - accuracy: 0.37600\n",
      "Iteration: 00023 - cost: 1.88179 - accuracy: 0.37800\n",
      "Iteration: 00024 - cost: 1.85928 - accuracy: 0.38200\n",
      "Iteration: 00025 - cost: 1.83644 - accuracy: 0.38200\n",
      "Iteration: 00026 - cost: 1.81327 - accuracy: 0.39800\n",
      "Iteration: 00027 - cost: 1.78986 - accuracy: 0.40800\n",
      "Iteration: 00028 - cost: 1.76641 - accuracy: 0.41600\n",
      "Iteration: 00029 - cost: 1.74298 - accuracy: 0.42000\n",
      "Iteration: 00030 - cost: 1.71963 - accuracy: 0.43200\n",
      "Iteration: 00031 - cost: 1.69644 - accuracy: 0.43400\n",
      "Iteration: 00032 - cost: 1.67346 - accuracy: 0.43600\n",
      "Iteration: 00033 - cost: 1.65077 - accuracy: 0.44000\n",
      "Iteration: 00034 - cost: 1.62838 - accuracy: 0.43800\n",
      "Iteration: 00035 - cost: 1.60633 - accuracy: 0.44200\n",
      "Iteration: 00036 - cost: 1.58468 - accuracy: 0.44400\n",
      "Iteration: 00037 - cost: 1.56349 - accuracy: 0.46000\n",
      "Iteration: 00038 - cost: 1.54273 - accuracy: 0.46200\n",
      "Iteration: 00039 - cost: 1.52243 - accuracy: 0.47200\n",
      "Iteration: 00040 - cost: 1.50259 - accuracy: 0.47000\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    x_train=X_train, \n",
    "    y_train=y_train, \n",
    "    x_test=X_test, \n",
    "    y_test=y_test, \n",
    "    epochs=40, \n",
    "    lr=0.005,\n",
    "    batch_size=512\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
