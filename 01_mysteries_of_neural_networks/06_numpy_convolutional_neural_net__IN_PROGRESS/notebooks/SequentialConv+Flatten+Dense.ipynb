{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Conv + Pool +Flatten + Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.activation.relu import ReluLayer\n",
    "from src.layers.pooling import MaxPoolLayer\n",
    "from src.activation.softmax import SoftmaxLayer\n",
    "from src.layers.dense import DenseLayer\n",
    "from src.layers.flatten import FlattenLayer\n",
    "from src.layers.convolutional import ConvLayer2D\n",
    "from src.model.sequential import SequentialModel\n",
    "from src.utils.core import convert_categorical2one_hot, convert_prob2categorical\n",
    "from src.utils.metrics import calculate_accuracy\n",
    "from src.optimizers.gradient_descent import GradientDescent\n",
    "from src.optimizers.rms_prop import RMSProp\n",
    "from src.optimizers.adam import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the train data set\n",
    "N_TRAIN_SAMPLES = 5000\n",
    "# number of samples in the test data set\n",
    "N_TEST_SAMPLES = 500\n",
    "# number of samples in the validation data set\n",
    "N_VALID_SAMPLES = 500\n",
    "# number of classes\n",
    "N_CLASSES = 10\n",
    "# image size\n",
    "IMAGE_SIZE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Build data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape: (60000, 28, 28)\n",
      "trainY shape: (60000,)\n",
      "testX shape: (10000, 28, 28)\n",
      "testY shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
    "print(\"trainX shape:\", trainX.shape)\n",
    "print(\"trainY shape:\", trainY.shape)\n",
    "print(\"testX shape:\", testX.shape)\n",
    "print(\"testY shape:\", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX[:N_TRAIN_SAMPLES, :, :]\n",
    "y_train = trainY[:N_TRAIN_SAMPLES]\n",
    "\n",
    "X_test = trainX[N_TRAIN_SAMPLES:N_TRAIN_SAMPLES+N_TEST_SAMPLES, :, :]\n",
    "y_test = trainY[N_TRAIN_SAMPLES:N_TRAIN_SAMPLES+N_TEST_SAMPLES]\n",
    "\n",
    "X_valid = testX[:N_VALID_SAMPLES, :, :]\n",
    "y_valid = testY[:N_VALID_SAMPLES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** We need to change the data format to the shape supported by my implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5000, 28, 28, 1)\n",
      "y_train shape: (5000, 10)\n",
      "X_test shape: (500, 28, 28, 1)\n",
      "y_test shape: (500, 10)\n",
      "X_valid shape: (500, 28, 28, 1)\n",
      "y_valid shape: (500, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train / 255\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "y_train = convert_categorical2one_hot(y_train)\n",
    "X_test = X_test / 255\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "y_test = convert_categorical2one_hot(y_test)\n",
    "X_valid = X_valid / 255\n",
    "X_valid = np.expand_dims(X_valid, axis=3)\n",
    "y_valid = convert_categorical2one_hot(y_valid)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    # input (N, 28, 28, 1) out (N, 26, 26, 16)\n",
    "    ConvLayer2D.initialize(filters=16, kernel_shape=(3, 3, 1), stride=1),\n",
    "    # input (N, 26, 26, 16) out (N, 26, 26, 16)\n",
    "    ReluLayer(),\n",
    "    # input (N, 26, 26, 16) out (N, 13, 13, 16)\n",
    "    MaxPoolLayer(pool_size=(2, 2), stride=2),\n",
    "    # input (N, 13, 13, 16) out (N, 9, 9, 32)\n",
    "    ConvLayer2D.initialize(filters=32, kernel_shape=(5, 5, 16), stride=1),\n",
    "    # input (N, 9, 9, 32) out (N, 9, 9, 32)\n",
    "    ReluLayer(),\n",
    "     # input (N, 9, 9, 32) out (N, 3, 3, 32)\n",
    "    MaxPoolLayer(pool_size=(3, 3), stride=3),\n",
    "    # input (N, 3, 3, 32) out (N, 288)\n",
    "    FlattenLayer(),\n",
    "    DenseLayer.initialize(units_prev=288, units_curr=256),\n",
    "    ReluLayer(),\n",
    "    DenseLayer.initialize(units_prev=256, units_curr=256),\n",
    "    ReluLayer(),\n",
    "    DenseLayer.initialize(units_prev=256, units_curr=128),\n",
    "    ReluLayer(),\n",
    "    DenseLayer.initialize(units_prev=128, units_curr=64),\n",
    "    ReluLayer(),\n",
    "    DenseLayer.initialize(units_prev=64, units_curr=N_CLASSES),\n",
    "    SoftmaxLayer()\n",
    "]\n",
    "\n",
    "optimizer = Adam(lr=0.003)\n",
    "\n",
    "model = SequentialModel(\n",
    "    layers=layers,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00001 - cost: 1.34659 - accuracy: 0.44800\n",
      "Iteration: 00002 - cost: 0.84036 - accuracy: 0.69400\n",
      "Iteration: 00003 - cost: 0.73173 - accuracy: 0.74400\n",
      "Iteration: 00004 - cost: 0.62193 - accuracy: 0.77800\n",
      "Iteration: 00005 - cost: 0.58773 - accuracy: 0.79000\n",
      "Iteration: 00006 - cost: 0.58484 - accuracy: 0.78800\n",
      "Iteration: 00007 - cost: 0.57690 - accuracy: 0.78000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1e044a0ded01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/Documents/REPOS/ILearnDeepLearning.py/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net__IN_PROGRESS/src/model/sequential.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, x_test, y_test, epochs, batch_size, test_frequency)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/REPOS/ILearnDeepLearning.py/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net__IN_PROGRESS/src/model/sequential.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda_curr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/REPOS/ILearnDeepLearning.py/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net__IN_PROGRESS/src/layers/convolutional.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, da_curr)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 self._dw += np.sum(\n\u001b[1;32m    128\u001b[0m                     \u001b[0ma_prev_pad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mh_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0mda_curr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    x_train=X_train, \n",
    "    y_train=y_train, \n",
    "    x_test=X_test, \n",
    "    y_test=y_test, \n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    test_frequency=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
